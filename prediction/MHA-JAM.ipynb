{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes import NuScenes\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.models as tvmodels\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import *\n",
    "# from torch.distributions.multivariate_normal import MultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DATAROOT = '../nu_data/v1.0-mini/'\n",
    "# nuscenes = NuScenes('v1.0-mini', dataroot=DATAROOT)\n",
    "# from nuscenes.eval.prediction.splits import get_prediction_challenge_split\n",
    "# mini_train = get_prediction_challenge_split(\"mini_train\", dataroot=DATAROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nuscenes.prediction import PredictHelper\n",
    "# helper = PredictHelper(nuscenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance_token, sample_token = mini_train[0].split(\"_\")\n",
    "# annotation = helper.get_annotations_for_sample('7626dde27d604ac28a0240bdd54eba7a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# future_xy_local = helper.get_future_for_agent(instance_token, sample_token, seconds=3, in_agent_frame=True)\n",
    "# future_xy_global = helper.get_future_for_agent(instance_token, sample_token, seconds=3, in_agent_frame=False)\n",
    "# helper.get_future_for_agent(instance_token, sample_token, seconds=3, in_agent_frame=True, just_xy=False)\n",
    "# sample = helper.get_annotations_for_sample(sample_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# from nuscenes.prediction.input_representation.static_layers import StaticLayerRasterizer\n",
    "# from nuscenes.prediction.input_representation.agents import AgentBoxesWithFadedHistory\n",
    "# from nuscenes.prediction.input_representation.interface import InputRepresentation\n",
    "# from nuscenes.prediction.input_representation.combinators import Rasterizer\n",
    "\n",
    "# static_layer_rasterizer = StaticLayerRasterizer(helper)\n",
    "# agent_rasterizer = AgentBoxesWithFadedHistory(helper, seconds_of_history=2)\n",
    "# mtp_input_representation = InputRepresentation(static_layer_rasterizer, agent_rasterizer, Rasterizer())\n",
    "\n",
    "# instance_token_img, sample_token_img = 'bc38961ca0ac4b14ab90e547ba79fbb6', '7626dde27d604ac28a0240bdd54eba7a'\n",
    "# anns = [ann for ann in nuscenes.sample_annotation if ann['instance_token'] == instance_token_img]\n",
    "# img = mtp_input_representation.make_input_representation(instance_token_img, sample_token_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import os\n",
    "# def sortKeyFunc(s):\n",
    "#     return int(os.path.basename(s)[0:-4])\n",
    "# video_dictionary = glob.glob(path + 'map_valid/*.png')\n",
    "# video_dictionary.sort(key=sortKeyFunc)\n",
    "# np.save('map_valid.npy', video_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/Nuscenes/MHA_JAM/'\n",
    "input_train = np.load(path + 'map_train.npy', allow_pickle=True)\n",
    "input_valid = np.load(path + 'map_valid.npy', allow_pickle=True)\n",
    "trajectory_train = np.load(path + 'trajectories_train.npy', allow_pickle=True)\n",
    "trajectory_valid = np.load(path + 'trajectories_valid.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(trajectory_train)\n",
    "num_valid = len(trajectory_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args():\n",
    "    def __init__(self):\n",
    "        self.top_len = 40\n",
    "        self.bot_len = 10\n",
    "        self.left_len = 25\n",
    "        self.right_len = 25\n",
    "        self.grid_size = 28\n",
    "        self.map_size = 50\n",
    "        self.input_size = 5\n",
    "        self.encoder_hidden_size = 64\n",
    "        self.embedding_size = 32\n",
    "        self.image_size = 500\n",
    "        self.image_out_size = 28\n",
    "        self.image_out_chanel = 512\n",
    "        self.head_n = 16\n",
    "        self.key_dim = 64\n",
    "        self.decoder_hidden_size = 128\n",
    "        self.prediction_time = 6 # 12s for prediction\n",
    "        self.decoder_output_size = 2 # 4 parameter define guassian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_input(relevent_traj_list, len_expected_history):\n",
    "#     '''\n",
    "#     1. padding the not enough history with oldest value\n",
    "#     2. padding none with 0\n",
    "#     '''\n",
    "#     input_li = list(np.array(list(relevent_traj_list[0].values())[0][0]))\n",
    "#     # print('input_li', input_li)\n",
    "#     for key, value in relevent_traj_list[1].items():\n",
    "        \n",
    "#         new_item = np.array(items[1][0])\n",
    "#         len_item = len(items[1][0])\n",
    "#         if len_item < len_expected_history:\n",
    "#             holder_item = np.zeros((len_expected_history, new_item.shape[1]))\n",
    "#             holder_item[len_expected_history - len_item:,:] = new_item\n",
    "#             for i in range(len_expected_history - len_item):\n",
    "#                 holder_item[i, :] = new_item[0]\n",
    "#             new_item = holder_item\n",
    "#         print(input_li)\n",
    "#         input_li.append(new_item[-len_expected_history:])\n",
    "#         print(np.array(new_item[-len_expected_history:]))\n",
    "#     return np.nan_to_num(np.array(input_li)).transpose((1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_coordinate(relevent_traj_list):\n",
    "    '''\n",
    "    format - self.O_corr [('bc38961ca0ac4b14ab90e547ba79fbb6', [392.945, 1148.426])]\n",
    "    format - self.T_corr [('bc38961ca0ac4b14ab90e547ba79fbb6', [392.945, 1148.426]), \n",
    "    # ('bc38961ca0ac4b14ab90e547ba79fbb6', [392.945, 1148.426])]\n",
    "    '''\n",
    "    if list(relevent_traj_list[0].values()) != [None]:\n",
    "        target_corr = [(i[0], i[1][0][-1][:2]) for i in relevent_traj_list[0].items()]\n",
    "        other_corr = [(i[0], i[1][0][-1][:2]) for i in relevent_traj_list[1].items()]\n",
    "        return target_corr, other_corr\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(relevent_traj_list):\n",
    "    past = []\n",
    "    future = []\n",
    "    for key, value in relevent_traj_list[0].items():\n",
    "        past.append(torch.from_numpy(np.nan_to_num(np.array(value[0])).astype(np.float32)))\n",
    "        future.append(torch.from_numpy(np.nan_to_num(np.array(value[1])).astype(np.float32)))\n",
    "    for key, value in relevent_traj_list[1].items():\n",
    "        past.append(torch.from_numpy(np.nan_to_num(np.array(value[0])).astype(np.float32)))\n",
    "        future.append(torch.from_numpy(np.nan_to_num(np.array(value[1])).astype(np.float32)))\n",
    "    return past, future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grid(args, target_corr, other_corr):\n",
    "    '''\n",
    "    return mask: shape[other_n, grid_size ** 2]\n",
    "    '''\n",
    "    top_len = args.top_len\n",
    "    bot_len = args.bot_len\n",
    "    left_len = args.left_len\n",
    "    right_len = args.right_len\n",
    "    grid_size = args.grid_size\n",
    "\n",
    "    target_x, target_y = target_corr[0][1]\n",
    "    height_low = target_y - bot_len\n",
    "    width_low = target_x - left_len\n",
    "    mask = np.zeros((len(other_corr), grid_size**2))\n",
    "    \n",
    "    idx = 0\n",
    "    for item in other_corr:\n",
    "        other_x, other_y = item[1]\n",
    "        cell_x = int(np.floor(((other_x - width_low)/ args.map_size) * grid_size))\n",
    "        cell_y = int(np.floor(((other_y - height_low)/ args.map_size) * grid_size))\n",
    "        mask[idx, cell_x + cell_y * grid_size] = 1\n",
    "        idx += 1\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialModel(nn.Module):\n",
    "    def __init__(self, args, infer=False):\n",
    "        super(SocialModel, self).__init__()\n",
    "        self.input_size = args.input_size\n",
    "        self.hidden_size = args.encoder_hidden_size\n",
    "        self.embedding_size = args.embedding_size\n",
    "        self.grid_size = args.grid_size\n",
    "        \n",
    "        self.input_embedding_layer = nn.Linear(self.input_size, self.embedding_size)\n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, batch_first=True)\n",
    "        \n",
    "    def get_social_tensor(self, grid, other_hidden_states):\n",
    "        '''\n",
    "        grid: shape[other_vehicle_n, grid_size ** 2]\n",
    "        hidden_states: shape[other_vehicle_n, hidden_size]\n",
    "        \n",
    "        return: shape[grid_size ** 2ï¼Œ hidden_size]\n",
    "        '''\n",
    "        other_vehicle_n = grid.size()[0]\n",
    "        social_tensor = torch.mm(torch.t(grid), other_hidden_states)\n",
    "        social_tensor = social_tensor.view(self.grid_size, self.grid_size, self.hidden_size)\n",
    "        return social_tensor.permute(2, 0, 1).unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x, grid, lens):\n",
    "        '''\n",
    "        x: shape[history_len (6s 12items), batch_size(other_n), state_n (5)]\n",
    "        grid: shape[other_vehicle_n, grid_size ** 2]\n",
    "        '''\n",
    "        x = self.input_embedding_layer(x)\n",
    "        packed_x = pack_padded_sequence(x, lens, batch_first=True, enforce_sorted=False)\n",
    "        packed_out, (hidden, cell) = self.lstm(packed_x)\n",
    "        target_hidden, other_hidden = hidden[0,0], hidden[0,1:]\n",
    "        return target_hidden, self.get_social_tensor(grid, other_hidden)# , embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mapmodel(args):\n",
    "    image_size = args.image_size\n",
    "    out_size = args.image_out_chanel\n",
    "    resnet50 = tvmodels.resnet50(pretrained=True)\n",
    "    resnet50 = nn.Sequential(*list(tvmodels.resnet50(pretrained=True).children())[:-3])\n",
    "    resnet50.output = nn.Conv2d(1024, 512, kernel_size=3)\n",
    "    resnet50.output_relu = nn.ReLU()\n",
    "    resnet50.output2 = nn.Conv2d(512, 512, kernel_size=3)\n",
    "    return resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderModel(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(DecoderModel, self).__init__()\n",
    "        self.prediction_time = args.prediction_time\n",
    "        self.decoder_hidden_size = args.decoder_hidden_size\n",
    "        self.encoder_hidden_size = args.encoder_hidden_size\n",
    "        self.decoder_output_size = args.decoder_output_size\n",
    "        # self.embedding_size = args.embedding_size\n",
    "        self.input_size = args.input_size\n",
    "        \n",
    "        # self.embedding_layer = input_embedding_layer\n",
    "        self.lstm1 = nn.LSTMCell(input_size=self.encoder_hidden_size\n",
    "                                 , hidden_size=self.decoder_hidden_size)\n",
    "        self.fc = nn.Linear(self.decoder_hidden_size, self.decoder_output_size)\n",
    "        \n",
    "    def forward(self, initial, attention_feature):\n",
    "        '''\n",
    "        most_recent_state [targetx, targety, 0,0,0,0]\n",
    "        '''\n",
    "        # initial_embed = self.embedding_layer(initial)\n",
    "        # initial_repeat = initial_embed.repeat(attention_feature.shape[0]).view(attention_feature.shape[0], -1)\n",
    "        # lstm_input = torch.cat((initial_repeat, attention_feature), 1)\n",
    "        hx = torch.randn(attention_feature.shape[0], self.decoder_hidden_size).to(device)\n",
    "        cx = torch.randn(attention_feature.shape[0], self.decoder_hidden_size).to(device)\n",
    "        output = []\n",
    "        \n",
    "        for i in range(self.prediction_time):\n",
    "            hx, cx = self.lstm1(attention_feature, (hx, cx))\n",
    "            temp_output = self.fc(hx)\n",
    "            output.append(temp_output)\n",
    "            # embed_output = self.embedding_layer(temp_output)\n",
    "            # lstm_input = torch.cat((embed_output, attention_feature), 1)\n",
    "            \n",
    "        return torch.stack(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VisualAttention, self).__init__()\n",
    "    def forward(self, query, key, value, args):\n",
    "        key = key.view(args.encoder_hidden_size, -1).squeeze()\n",
    "        QK = torch.mm(query, key).view(-1)\n",
    "        alpha = F.softmax(QK / np.sqrt(args.key_dim)).view(args.image_out_size, args.image_out_size)\n",
    "        a = alpha * value\n",
    "        return torch.sum(a, dim=(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA_JAM(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(MHA_JAM, self).__init__()\n",
    "        \n",
    "        self.args = args\n",
    "        self.image_out_size = args.image_out_size\n",
    "        self.image_out_channel = args.image_out_chanel\n",
    "        self.head_n = args.head_n\n",
    "        self.key_dim = args.key_dim\n",
    "        self.encoder_hidden_size = args.encoder_hidden_size\n",
    "        \n",
    "        self.social_model = SocialModel(args)\n",
    "        self.map_model = Mapmodel(args)\n",
    "        self.decoder_model = DecoderModel(args) # Not Implemented\n",
    "        \n",
    "        # self.gen_query = [nn.Linear(self.encoder_hidden_size, self.encoder_hidden_size * ) for i in range(self.head_n)]\n",
    "        # self.gen_key = [nn.Conv2d(self.image_out_channel + self.encoder_hidden_size, self.key_dim, kernel_size=1) \n",
    "        #                 for i in range(self.head_n)]\n",
    "        # self.gen_value = [nn.Conv2d(self.image_out_channel + self.encoder_hidden_size, self.key_dim, kernel_size=1) \n",
    "        #                  for i in range(self.head_n)]\n",
    "        # self.attentions = [VisualAttention() for i in range(self.head_n)]\n",
    "        \n",
    "        self.gen_query = nn.Linear(self.encoder_hidden_size, self.encoder_hidden_size * self.head_n)\n",
    "        self.gen_key = nn.Conv2d(self.image_out_channel + self.encoder_hidden_size, \n",
    "                                 self.key_dim * self.head_n, kernel_size=1)\n",
    "        self.gen_value = nn.Conv2d(self.image_out_channel + self.encoder_hidden_size, \n",
    "                                   self.key_dim * self.head_n, kernel_size=1)\n",
    "        self.attentions = [VisualAttention() for i in range(self.head_n)]\n",
    "        \n",
    "        self.p_prediction_fc1 = nn.Linear(self.encoder_hidden_size*self.head_n, self.head_n)\n",
    "        # self.p_prediction_fc2 = nn.Linear(int(self.encoder_hidden_size / 2), 1)\n",
    "        \n",
    "    def forward(self, map_image, grid, trajectory_data, lens):\n",
    "        '''\n",
    "        map_image is shape(1, 3, 500, 500)\n",
    "        grid tell the occupancy info around the vehicle\n",
    "        trajectory_data tell the trajectory we use \n",
    "        '''\n",
    "        map_feature = self.map_model(map_image)\n",
    "        target_hidden, social_feature = self.social_model(trajectory_data, grid, lens)\n",
    "        social_feature = torch.flip(social_feature, [2])\n",
    "        #??? the map and socal may have different direction \n",
    "        concat_feature = torch.cat((map_feature, social_feature), 1)\n",
    "        query = self.gen_query(target_hidden).unsqueeze(0)\n",
    "        key = self.gen_key(concat_feature)\n",
    "        value = self.gen_value(concat_feature)\n",
    "        attention_feature_list = []\n",
    "        for i in range(self.head_n): # [1, 1024, 28, 28]\n",
    "            query_i = query[:, i * self.encoder_hidden_size: (i+1) * self.encoder_hidden_size]\n",
    "            key_i = key[:, i * self.key_dim: (i+1) * self.key_dim ,: ,:]\n",
    "            value_i = value[:, i * self.key_dim: (i+1) * self.key_dim ,: ,:]\n",
    "            attention_feature_list.append(target_hidden + self.attentions[i](query_i, key_i, value_i, self.args))\n",
    "        \n",
    "        initial = trajectory_data[0][0]\n",
    "        cated_attention_feature = torch.cat(attention_feature_list)\n",
    "#         cated_attention_feature_for_pred = cated_attention_feature.clone()\n",
    "        output = self.decoder_model(initial, cated_attention_feature)\n",
    "        # for probability prediction for each interaction\n",
    "        p_pred = F.softmax(self.p_prediction_fc1(cated_attention_feature.view(-1)), dim=0)\n",
    "        # p_pred = self.p_prediction_fc2(p_pred)\n",
    "        return output, p_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # initialize all the hyper-parameter\n",
    "# a = args()\n",
    "# # get all the agent's coordination in this timestep\n",
    "# target_corr, other_corr = get_current_coordinate(relevent_traj_list = \n",
    "#                        agent_rasterizer.extract_relevant_trajectory(instance_token_img, sample_token_img))\n",
    "# # for a given time and agent, compute the occupancy grid\n",
    "# grid = compute_grid(a, target_corr, other_corr)\n",
    "# # compute the input history data\n",
    "# relevent_traj_list = agent_rasterizer.extract_relevant_trajectory(instance_token_img, sample_token_img)\n",
    "# input_data = format_input(relevent_traj_list, 5)\n",
    "# # compute input image\n",
    "# img = mtp_input_representation.make_input_representation(instance_token_img, sample_token_img)\n",
    "# # model forward\n",
    "# model = MHA_JAM(a)\n",
    "# output, p_pred = model(torch.tensor(img).permute(2, 0, 1).unsqueeze(0).float(), \n",
    "#       torch.tensor(grid).float(), \n",
    "#       torch.tensor(input_data).float())\n",
    "# p_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_multivariate_Gaussian(output, k=2, alpha=0.05, eps=1e-3):\n",
    "#     \"\"\"\n",
    "#     Format neural network output into mean and covariance\n",
    "#     Args:\n",
    "#     output : (k*(k + 3)/2) tensor\n",
    "#     k : integer dimension of regression problem\n",
    "#     alpha : saturation coefficient\n",
    "#     eps : small value for numerical stability\n",
    "#     \"\"\"\n",
    "#     assert output.shape[0] == k*(k + 3)//2\n",
    "#     mean = output[:k]\n",
    "#     var = output[k:2*k].exp()\n",
    "#     var_mat = torch.sqrt(var.unsqueeze(1)*var.unsqueeze(0))\n",
    "#     # Build correlation matrix\n",
    "#     rhos = (1 - eps)*torch.tanh(alpha*output[2*k:])\n",
    "#     rho_mat = torch.ones_like(var_mat)\n",
    "#     for i in range(k):\n",
    "#         for j in range(i + 1, k):\n",
    "#             # Flattened upper diagonal indexing\n",
    "#             rho = rhos[k*i - i*(i + 3)//2 + j - 1]\n",
    "#             rho_mat[i, j] = rho\n",
    "#             rho_mat[j, i] = rho\n",
    "#     return mean, rho_mat*var_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_backward(input_data, target, p):\n",
    "#     '''\n",
    "#     input data (head_n, timestep, 6): each timestep 6 dimension (mu_x, mu_y, v_x, v_y)\n",
    "#     target data (timestep, 2): each timestep 2 dimension (x, y)\n",
    "#     p (head_n): \n",
    "#     '''\n",
    "#     sum_prob = []\n",
    "#     min_idx = 0\n",
    "#     for head_idx in range(input_data.shape[0]):\n",
    "        \n",
    "# #         dist = [MultivariateNormal(input_data[head_idx, i, :2], \n",
    "# #                                    torch.diag(input_data[head_idx, i, 2:4].exp())) for i in range(input_data.shape[1])]\n",
    "#         dist = []\n",
    "#         for i in range(input_data.shape[1]):\n",
    "#             mean, cov = process_multivariate_Gaussian(input_data[head_idx, i])\n",
    "#             dist.append(MultivariateNormal(mean, cov))\n",
    "        \n",
    "#         prob = torch.stack([- dist[i].log_prob(target[i, :]) \n",
    "#                             for i in range(input_data.shape[1])])\n",
    "#         sum_prob.append(prob.sum())\n",
    "        \n",
    "#     min_val = torch.min(torch.stack(sum_prob))\n",
    "#     min_idx = torch.argmin(torch.stack(sum_prob))\n",
    "#     prob = - torch.log(p[min_idx] + 1e-7)\n",
    "#     loss = min_val + prob\n",
    "#     return loss, input_data[min_idx,:,:2].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_backward(input_data, target, p):\n",
    "    '''\n",
    "    input data (head_n, timestep, 4): each timestep 4 dimension (mu_x, mu_y, v_x, v_y)\n",
    "    target data (timestep, 2): each timestep 2 dimension (x, y)\n",
    "    p (head_n): \n",
    "    '''\n",
    "#     mean, logvar = input_data[:,:,0:2], input_data[:,:,2:4]\n",
    "#     mse_loss = (torch.square(mean - target)*torch.exp(-logvar)).sum(dim=-1).sum(dim=-1)\n",
    "#     var_loss = logvar.sum(dim=-1).sum(dim=-1)\n",
    "#     loss = mse_loss + var_loss\n",
    "    \n",
    "    mse = torch.square(input_data - target).sum(dim=-1).sum(dim=-1)\n",
    "    loss = mse\n",
    "    loss_min = torch.min(loss)\n",
    "    loss_index = torch.argmin(loss)\n",
    "    \n",
    "    prob = - torch.log(p[loss_index] + 1e-7)\n",
    "    loss = loss_min + prob\n",
    "    return loss, input_data[loss_index,:,:2].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "num_epoch = 100\n",
    "a = args()\n",
    "model = MHA_JAM(a)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "# model.load_state_dict(torch.load('MHA_JAM.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_epoch_train = []\n",
    "loss_epoch_valid = []\n",
    "for epoch in range(num_epoch):\n",
    "    index = np.random.permutation(num_train)\n",
    "    running_loss = 0\n",
    "    for n in range(num_train):\n",
    "        model.train()\n",
    "        i = index[n]\n",
    "        target_corr, other_corr = get_current_coordinate(relevent_traj_list = trajectory_train[i])\n",
    "        if target_corr == None:\n",
    "            print('skip', i)\n",
    "            continue\n",
    "        # for a given time and agent, compute the occupancy grid\n",
    "        grid = compute_grid(a, target_corr, other_corr)\n",
    "        # compute the input history data\n",
    "#         length = len(list(trajectory_train[i][0].values())[0])\n",
    "#         trajectory = format_input(trajectory_train[i], length)\n",
    "        trajectory, targets = format_input(trajectory_train[i])\n",
    "        # compute input image\n",
    "        lengths = torch.LongTensor([len(item) for item in trajectory])\n",
    "        trajectory = pad_sequence(trajectory, batch_first=True)\n",
    "        \n",
    "        image = plt.imread(input_train[i])\n",
    "        # model forward\n",
    "\n",
    "        output, p_pred = model(torch.FloatTensor(image).permute(2, 0, 1).unsqueeze(0).to(device), \n",
    "                               torch.FloatTensor(grid).to(device),\n",
    "                               trajectory.to(device),\n",
    "                               lengths.to(device))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss, best_prediction = loss_backward(output, targets[0][0:6,0:2].to(device), p_pred)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.detach().cpu().numpy()\n",
    "        \n",
    "        if (n+1) % 1000 == 0:\n",
    "            print(epoch+1, n+1, loss.detach().cpu().numpy(), i+1)\n",
    "        \n",
    "    running_loss /= num_train\n",
    "    loss_epoch_train.append(running_loss)\n",
    "    torch.save(model.state_dict(), 'MHA_JAM_regression.pth')\n",
    "    print('Train_loss:', running_loss)\n",
    "    \n",
    "    print('validation')\n",
    "    running_loss = 0\n",
    "    for i in range(num_valid):\n",
    "        model.eval()\n",
    "        target_corr, other_corr = get_current_coordinate(relevent_traj_list = trajectory_valid[i])\n",
    "        if target_corr == None:\n",
    "            print('skip')\n",
    "            continue\n",
    "        # for a given time and agent, compute the occupancy grid\n",
    "        grid = compute_grid(a, target_corr, other_corr)\n",
    "        # compute the input history data\n",
    "#         length = len(list(trajectory_train[i][0].values())[0])\n",
    "#         trajectory = format_input(trajectory_train[i], length)\n",
    "        trajectory, targets = format_input(trajectory_valid[i])\n",
    "        # compute input image\n",
    "        lengths = torch.LongTensor([len(item) for item in trajectory])\n",
    "        trajectory = pad_sequence(trajectory, batch_first=True)\n",
    "        \n",
    "        image = plt.imread(input_valid[i])\n",
    "        # model forward\n",
    "        with torch.no_grad():\n",
    "            output, p_pred = model(torch.FloatTensor(image).permute(2, 0, 1).unsqueeze(0).to(device), \n",
    "                               torch.FloatTensor(grid).to(device),\n",
    "                               trajectory.to(device),\n",
    "                               lengths.to(device))\n",
    "\n",
    "            loss, best_prediction = loss_backward(output, targets[0][0:6,0:2].to(device), p_pred)\n",
    "            running_loss += loss.detach().cpu().numpy()\n",
    "            \n",
    "            if (i+1) % 1000 == 0:\n",
    "                print(epoch+1, i+1, loss.detach().cpu().numpy())\n",
    "            \n",
    "#         if (i+1) % 10 == 0:\n",
    "#             plt.scatter(best_prediction[:,0], best_prediction[:,1], color='r')\n",
    "#             plt.scatter(target_valid[i,:,0], target_valid[i,:,1], color='b')\n",
    "#             plt.show()\n",
    "    running_loss /= num_valid\n",
    "    loss_epoch_valid.append(running_loss)\n",
    "    print('Valid_loss:', running_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_valid):\n",
    "    target_corr, other_corr = get_current_coordinate(relevent_traj_list = trajectory_valid[i])\n",
    "    if target_corr == None:\n",
    "        print('skip')\n",
    "        continue\n",
    "    # for a given time and agent, compute the occupancy grid\n",
    "    grid = compute_grid(a, target_corr, other_corr)\n",
    "    # compute the input history data\n",
    "#         length = len(list(trajectory_train[i][0].values())[0])\n",
    "#         trajectory = format_input(trajectory_train[i], length)\n",
    "    trajectory, targets = format_input(trajectory_valid[i])\n",
    "    # compute input image\n",
    "    lengths = torch.LongTensor([len(item) for item in trajectory])\n",
    "    trajectory = pad_sequence(trajectory, batch_first=True)\n",
    "\n",
    "    image = plt.imread(input_valid[i])\n",
    "    # model forward\n",
    "    with torch.no_grad():\n",
    "        output, p_pred = model(torch.FloatTensor(image).permute(2, 0, 1).unsqueeze(0).to(device), \n",
    "                           torch.FloatTensor(grid).to(device),\n",
    "                           trajectory.to(device),\n",
    "                           lengths.to(device))\n",
    "\n",
    "        loss, best_prediction = loss_backward(output, targets[0][:,0:2].to(device), p_pred)\n",
    "        running_loss += loss.detach().cpu().numpy()\n",
    "\n",
    "        if (i+1) % 1000 == 0:\n",
    "            print(epoch+1, i+1, loss.detach().cpu().numpy())\n",
    "            \n",
    "        if (i+1) % 10 == 0:\n",
    "            plt.scatter(best_prediction[:,0], best_prediction[:,1], color='r')\n",
    "            plt.scatter(targets[0][:,0], targets[0][:,1], color='b')\n",
    "            plt.show()\n",
    "    running_loss /= num_valid\n",
    "    loss_epoch_valid.append(running_loss)\n",
    "    print('Valid_loss:', running_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for i in range(num_train):\n",
    "# i = 0\n",
    "# position = []\n",
    "# for key, value in trajectory_train[i][0].items():\n",
    "#     for n in range(len(trajectory_train[i][0][key])):\n",
    "#         position.append(trajectory_train[i][0][key][n][0:2])\n",
    "# position = np.array(position)\n",
    "# position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(position[:,0], position[:,1], color='r')\n",
    "# plt.plot(target_train[i,:,0], target_train[i,:,1], color='b')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Social tensor visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm\n",
    "import matplotlib.colorbar\n",
    "import matplotlib.colors\n",
    "\n",
    "def cuboid_data(center, size=(1,1,1)):\n",
    "    # code taken from\n",
    "    # http://stackoverflow.com/questions/30715083/python-plotting-a-wireframe-3d-cuboid?noredirect=1&lq=1\n",
    "    # suppose axis direction: x: to left; y: to inside; z: to upper\n",
    "    # get the (left, outside, bottom) point\n",
    "    o = [a - b / 2 for a, b in zip(center, size)]\n",
    "    # get the length, width, and height\n",
    "    l, w, h = size\n",
    "    x = [[o[0], o[0] + l, o[0] + l, o[0], o[0]],  # x coordinate of points in bottom surface\n",
    "         [o[0], o[0] + l, o[0] + l, o[0], o[0]],  # x coordinate of points in upper surface\n",
    "         [o[0], o[0] + l, o[0] + l, o[0], o[0]],  # x coordinate of points in outside surface\n",
    "         [o[0], o[0] + l, o[0] + l, o[0], o[0]]]  # x coordinate of points in inside surface\n",
    "    y = [[o[1], o[1], o[1] + w, o[1] + w, o[1]],  # y coordinate of points in bottom surface\n",
    "         [o[1], o[1], o[1] + w, o[1] + w, o[1]],  # y coordinate of points in upper surface\n",
    "         [o[1], o[1], o[1], o[1], o[1]],          # y coordinate of points in outside surface\n",
    "         [o[1] + w, o[1] + w, o[1] + w, o[1] + w, o[1] + w]]    # y coordinate of points in inside surface\n",
    "    z = [[o[2], o[2], o[2], o[2], o[2]],                        # z coordinate of points in bottom surface\n",
    "         [o[2] + h, o[2] + h, o[2] + h, o[2] + h, o[2] + h],    # z coordinate of points in upper surface\n",
    "         [o[2], o[2], o[2] + h, o[2] + h, o[2]],                # z coordinate of points in outside surface\n",
    "         [o[2], o[2], o[2] + h, o[2] + h, o[2]]]                # z coordinate of points in inside surface\n",
    "    return np.array(x), np.array(y), np.array(z)\n",
    "\n",
    "def plotCubeAt(pos=(0,0,0), c=\"w\", alpha=0.1, ax=None):\n",
    "    # Plotting N cube elements at position pos\n",
    "    if ax !=None:\n",
    "        X, Y, Z = cuboid_data( (pos[0],pos[1],pos[2]) )\n",
    "        # print(c)\n",
    "        ax.plot_surface(X, Y, Z, color=c, rstride=1, cstride=1, alpha=alpha)\n",
    "\n",
    "def plotMatrix(ax, x, y, z, data, cmap=plt.cm.get_cmap('PiYG'), cax=None, alpha=0.1):\n",
    "    # plot a Matrix \n",
    "    norm = matplotlib.colors.Normalize(vmin=-0.2, vmax=0.2)\n",
    "    colors = lambda i,j,k : (1,1,1,alpha) if (0.01<=data[i,j,k]<= 0.01) else matplotlib.cm.ScalarMappable(norm=norm,cmap = cmap).to_rgba(data[i,j,k])\n",
    "    #print(colors)\n",
    "    for i, xi in enumerate(x):\n",
    "            for j, yi in enumerate(y):\n",
    "                for k, zi, in enumerate(z):\n",
    "                    # print(colors(i,j,k))\n",
    "                    plotCubeAt(pos=(xi, yi, zi), c=colors(i,j,k), alpha=alpha,  ax=ax)\n",
    "\n",
    "\n",
    "\n",
    "    if cax !=None:\n",
    "        cbar = matplotlib.colorbar.ColorbarBase(cax, cmap=plt.cm.get_cmap('bwr'), norm=norm, orientation='vertical')  \n",
    "        cbar.set_ticks(np.unique(data))\n",
    "        # set the colorbar transparent as well\n",
    "        cbar.solids.set(alpha=alpha)              \n",
    "\n",
    "# x and y and z coordinates\n",
    "x = np.array(range(28))\n",
    "y = np.array(range(28))\n",
    "z = np.array(range(64))\n",
    "data = social.detach().numpy()\n",
    "\n",
    "data[data < 0] = -0.2\n",
    "data[data > 0] = 0.2\n",
    "data[data == 0] = 0\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_axes([0.1, 0.1, 0.7, 0.8], projection='3d')\n",
    "ax_cb = fig.add_axes([0.8, 0.7, 0.05, 0.45])\n",
    "\n",
    "plotMatrix(ax, x, y, z, data[:28,:28,:64], cmap=\"bwr\", cax = ax_cb)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "for n in range(num_valid):\n",
    "    target_corr, other_corr = get_current_coordinate(relevent_traj_list = trajectory_valid[n])\n",
    "    if target_corr == None:\n",
    "        t.append(n)\n",
    "        print('skip', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
